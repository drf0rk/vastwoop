{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VASTWOOP (ROOP-FLOYD) Setup\n",
    "\n",
    "This notebook will set up the environment for VASTWOOP (formerly ROOP-FLOYD) with proper CUDA and PyTorch configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 1: Install system dependencies\n",
    "!apt update -y && apt install -y git python3-pip ffmpeg --no-install-recommends\n",
    "\n",
    "# Step 2: Clone repository\n",
    "import os\n",
    "repo_dir = \"/vastwoop\"\n",
    "if not os.path.exists(repo_dir):\n",
    "    !git clone https://github.com/drf0rk/vastwoop.git {repo_dir}\n",
    "\n",
    "# Step 3: Set CUDA environment variables - critical for compatibility\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-11.8\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"/usr/local/cuda/compat/lib.real:{os.environ['CUDA_HOME']}/lib64:/usr/lib/x86_64-linux-gnu:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "os.environ[\"PATH\"] = f\"{os.environ['CUDA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "os.environ[\"ORT_TENSORRT_ENGINE_CACHE_ENABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 4: Clean installation of PyTorch and onnxruntime to avoid conflicts\n",
    "!pip uninstall -y torch torchvision onnxruntime onnxruntime-gpu\n",
    "!pip install --no-cache-dir torch==2.1.0+cu118 torchvision==0.16.0+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install --no-cache-dir onnxruntime-gpu==1.16.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5: Create import_helper.py for proper import order\n",
    "%%writefile {repo_dir}/import_helper.py\n",
    "# Ensure torch is imported before onnxruntime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['ORT_TENSORRT_ENGINE_CACHE_ENABLE'] = '1'\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-11.8\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = f\"/usr/local/cuda/compat/lib.real:{os.environ['CUDA_HOME']}/lib64:/usr/lib/x86_64-linux-gnu:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "# Import torch first\n",
    "import torch\n",
    "print(f\"PyTorch {torch.__version__} loaded successfully\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Then import onnxruntime\n",
    "import onnxruntime as ort\n",
    "print(f\"ONNX Runtime {ort.__version__} loaded successfully\")\n",
    "print(f\"Available providers: {ort.get_available_providers()}\")\n",
    "print(f\"Device: {ort.get_device()}\")\n",
    "\n",
    "# Export function to get optimal providers\n",
    "def get_providers():\n",
    "    return ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 6: Create requirements.txt with properly ordered dependencies\n",
    "%%writefile {repo_dir}/requirements.txt\n",
    "--extra-index-url https://download.pytorch.org/whl/cu118\n",
    "numpy==1.26.4\n",
    "torch==2.1.0+cu118\n",
    "torchvision==0.16.0+cu118\n",
    "onnxruntime-gpu==1.16.3\n",
    "gradio==5.9.1\n",
    "opencv-python-headless==4.10.0.84\n",
    "onnx==1.16.1\n",
    "insightface==0.7.3\n",
    "albucore==0.0.16\n",
    "psutil==5.9.6\n",
    "tqdm==4.66.4\n",
    "ftfy\n",
    "regex\n",
    "pyvirtualcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 7: Install remaining dependencies\n",
    "%cd {repo_dir}\n",
    "!pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Additional specific dependency fixes\n",
    "!pip install --upgrade gradio --force\n",
    "!pip install --upgrade fastapi pydantic\n",
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 8: Create JupyterLab navigation helper\n",
    "%%writefile {repo_dir}/jupyter_utils.py\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def goto_vastwoop():\n",
    "    \"\"\"Navigate to the VASTWOOP directory\"\"\"\n",
    "    vastwoop_dir = \"/vastwoop\"\n",
    "    if os.path.exists(vastwoop_dir):\n",
    "        os.chdir(vastwoop_dir)\n",
    "        print(f\"Changed directory to: {vastwoop_dir}\")\n",
    "        \n",
    "        # Make sure import_helper is loaded first\n",
    "        if vastwoop_dir not in sys.path:\n",
    "            sys.path.insert(0, vastwoop_dir)\n",
    "            \n",
    "        # Force import of torch before onnxruntime\n",
    "        try:\n",
    "            from import_helper import torch, ort, get_providers\n",
    "            print(\"Environment verified: torch loaded before onnxruntime\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"Directory not found: {vastwoop_dir}\")\n",
    "        return False\n",
    "\n",
    "def goto_root():\n",
    "    \"\"\"Navigate to the root directory\"\"\"\n",
    "    os.chdir(\"/\")\n",
    "    print(\"Changed directory to root: /\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 9: Create error_checker.py for diagnostics\n",
    "%%writefile {repo_dir}/error_checker.py\n",
    "# Error checking and diagnostics for VASTWOOP\n",
    "import os\n",
    "import sys\n",
    "import importlib.util\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "def check_gpu():\n",
    "    \"\"\"Check if GPU is available and working\"\"\"\n",
    "    try:\n",
    "        # Ensure torch is imported first\n",
    "        import torch\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        \n",
    "        if not cuda_available:\n",
    "            print(\"CUDA not available. Checking possible issues...\")\n",
    "            \n",
    "            # Check if CUDA_HOME is set\n",
    "            cuda_home = os.environ.get(\"CUDA_HOME\")\n",
    "            if not cuda_home:\n",
    "                print(\"CUDA_HOME environment variable not set\")\n",
    "                print(\"Try setting: os.environ['CUDA_HOME'] = '/usr/local/cuda-11.8'\")\n",
    "            else:\n",
    "                print(f\"CUDA_HOME is set to: {cuda_home}\")\n",
    "                \n",
    "            # Check if LD_LIBRARY_PATH is correctly set\n",
    "            ld_path = os.environ.get(\"LD_LIBRARY_PATH\")\n",
    "            if not ld_path or \"/usr/local/cuda\" not in ld_path:\n",
    "                print(\"LD_LIBRARY_PATH may not be set correctly\")\n",
    "                print(\"Try setting: os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/compat/lib.real:/usr/local/cuda-11.8/lib64:/usr/lib/x86_64-linux-gnu'\")\n",
    "            else:\n",
    "                print(f\"LD_LIBRARY_PATH includes CUDA paths: {ld_path}\")\n",
    "                \n",
    "            # Try to run nvidia-smi\n",
    "            try:\n",
    "                result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "                if result.returncode != 0:\n",
    "                    print(\"nvidia-smi failed, indicating NVIDIA driver issues:\")\n",
    "                    print(result.stderr)\n",
    "                else:\n",
    "                    print(\"nvidia-smi successful, but PyTorch still can't detect CUDA\")\n",
    "                    print(\"This may indicate PyTorch/CUDA version mismatch\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error running nvidia-smi: {e}\")\n",
    "                print(\"NVIDIA drivers may not be installed or functioning correctly\")\n",
    "            \n",
    "            return False, None, None\n",
    "        else:\n",
    "            device_name = torch.cuda.get_device_name(0)\n",
    "            device_cap = torch.cuda.get_device_capability(0)\n",
    "            cuda_version = torch.version.cuda\n",
    "            \n",
    "            print(f\"CUDA is available: {cuda_available}\")\n",
    "            print(f\"Device: {device_name}\")\n",
    "            print(f\"Compute Capability: {device_cap}\")\n",
    "            print(f\"CUDA Version: {cuda_version}\")\n",
    "            \n",
    "            return True, device_name, cuda_version\n",
    "    except ImportError:\n",
    "        print(\"Could not import torch. Please install PyTorch first.\")\n",
    "        return False, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking GPU: {e}\")\n",
    "        return False, None, None\n",
    "\n",
    "def check_onnx_providers():\n",
    "    \"\"\"Check if ONNX Runtime providers are available\"\"\"\n",
    "    try:\n",
    "        # Import torch first\n",
    "        import torch\n",
    "        # Then import onnxruntime\n",
    "        import onnxruntime as ort\n",
    "        \n",
    "        providers = ort.get_available_providers()\n",
    "        print(f\"Available ONNX Runtime providers: {providers}\")\n",
    "        \n",
    "        if 'CUDAExecutionProvider' not in providers:\n",
    "            print(\"WARNING: CUDA Execution Provider not available for ONNX Runtime\")\n",
    "            print(\"This may significantly impact performance\")\n",
    "            print(\"Make sure you have installed onnxruntime-gpu and imported torch before onnxruntime\")\n",
    "            \n",
    "        return providers\n",
    "    except ImportError as e:\n",
    "        print(f\"Import error: {e}\")\n",
    "        print(\"Please install PyTorch and ONNX Runtime first\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking ONNX Runtime providers: {e}\")\n",
    "        return []\n",
    "\n",
    "def check_dependencies():\n",
    "    \"\"\"Check if all required dependencies are installed\"\"\"\n",
    "    dependencies = [\n",
    "        'torch', 'torchvision', 'onnxruntime', 'numpy', 'gradio', \n",
    "        'opencv-python-headless', 'insightface', 'onnx'\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    for dep in dependencies:\n",
    "        pkg_name = dep.split('-')[0]  # Handle opencv-python-headless\n",
    "        try:\n",
    "            if pkg_name == 'onnxruntime':\n",
    "                # Check if it's onnxruntime-gpu\n",
    "                try:\n",
    "                    import onnxruntime as ort\n",
    "                    gpu_available = 'CUDAExecutionProvider' in ort.get_available_providers()\n",
    "                    version = ort.__version__\n",
    "                    results[dep] = (True, version, gpu_available)\n",
    "                except ImportError:\n",
    "                    results[dep] = (False, None, False)\n",
    "            else:\n",
    "                module = importlib.import_module(pkg_name)\n",
    "                version = getattr(module, '__version__', 'Unknown')\n",
    "                results[dep] = (True, version, None)\n",
    "        except ImportError:\n",
    "            results[dep] = (False, None, None)\n",
    "    \n",
    "    print(\"\\nDependency Check Results:\")\n",
    "    all_installed = True\n",
    "    for dep, (installed, version, gpu) in results.items():\n",
    "        status = f\"v{version}\" if installed else \"NOT INSTALLED\"\n",
    "        if gpu is not None:\n",
    "            status += f\" (GPU: {'Available' if gpu else 'Not Available'})\"\n",
    "        print(f\"{dep}: {status}\")\n",
    "        all_installed = all_installed and installed\n",
    "    \n",
    "    return all_installed, results\n",
    "\n",
    "def run_diagnostics():\n",
    "    \"\"\"Run all diagnostics and return overall status\"\"\"\n",
    "    print(\"=== VASTWOOP System Diagnostics ===\\n\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Platform: {platform.platform()}\")\n",
    "    print(\"\\n=== GPU Check ===\")\n",
    "    gpu_available, device_name, cuda_version = check_gpu()\n",
    "    print(\"\\n=== ONNX Runtime Check ===\")\n",
    "    providers = check_onnx_providers()\n",
    "    print(\"\\n=== Dependency Check ===\")\n",
    "    all_deps_installed, dep_results = check_dependencies()\n",
    "    \n",
    "    # Overall status\n",
    "    print(\"\\n=== Overall Status ===\")\n",
    "    cuda_ok = gpu_available and cuda_version is not None\n",
    "    onnx_ok = 'CUDAExecutionProvider' in providers\n",
    "    \n",
    "    if cuda_ok and onnx_ok and all_deps_installed:\n",
    "        print(\"✅ All systems operational\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"⚠️ Some issues detected. See details above.\")\n",
    "        return False\n",
    "\n",
    "# Function to run when imported\n",
    "def ensure_torch_first():\n",
    "    \"\"\"Ensure torch is imported before onnxruntime\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        import onnxruntime as ort\n",
    "        return True\n",
    "    except ImportError as e:\n",
    "        print(f\"Import error: {e}\")\n",
    "        return False\n",
    "\n",
    "# If this script is run directly\n",
    "if __name__ == \"__main__\":\n",
    "    run_diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 10: Fix run.py to ensure proper import order\n",
    "if os.path.exists(f\"{repo_dir}/run.py\"):\n",
    "    with open(f\"{repo_dir}/run.py\", \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Only modify if it doesn't already have the import\n",
    "    if \"from import_helper import\" not in content:\n",
    "        new_content = f\"# Import torch before onnxruntime\\nfrom import_helper import torch, ort, get_providers\\n\\n{content}\"\n",
    "        with open(f\"{repo_dir}/run.py\", \"w\") as f:\n",
    "            f.write(new_content)\n",
    "        print(\"Modified run.py to import torch before onnxruntime\")\n",
    "    else:\n",
    "        print(\"run.py already has the proper import order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 11: Verify final installation\n",
    "from import_helper import torch, ort, get_providers\n",
    "print(\"\\n=== Final Environment Check ===\")\n",
    "print(f\"PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}, Available: {torch.cuda.is_available()}\")\n",
    "print(f\"onnxruntime: {ort.__version__}, Device: {ort.get_device()}\")\n",
    "print(f\"Recommended providers: {get_providers()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 12: Run full diagnostics\n",
    "import sys\n",
    "sys.path.insert(0, repo_dir)\n",
    "from error_checker import run_diagnostics\n",
    "run_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "Now that setup is complete, you can use the following in your code:\n",
    "\n",
    "```python\n",
    "from import_helper import torch, ort, get_providers\n",
    "```\n",
    "\n",
    "To navigate around JupyterLab:\n",
    "```python\n",
    "from jupyter_utils import goto_vastwoop, goto_root\n",
    "goto_vastwoop()  # Go to VASTWOOP directory\n",
    "```\n",
    "\n",
    "To run diagnostics:\n",
    "```python\n",
    "from error_checker import run_diagnostics\n",
    "run_diagnostics()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 13: Create startup script\n",
    "%%writefile {repo_dir}/startup.py\n",
    "#!/usr/bin/env python3\n",
    "# VASTWOOP startup script\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Add current directory to path\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "# Import the helper module to ensure torch is loaded before onnxruntime\n",
    "try:\n",
    "    from import_helper import torch, ort, get_providers\n",
    "    from error_checker import run_diagnostics\n",
    "    \n",
    "    # Run basic diagnostics\n",
    "    print(\"Running system checks...\")\n",
    "    all_ok = run_diagnostics()\n",
    "    \n",
    "    if all_ok:\n",
    "        print(\"\\nStarting VASTWOOP...\")\n",
    "        # Launch the main application\n",
    "        import run\n",
    "    else:\n",
    "        print(\"\\nSystem checks failed. Please fix the issues before running VASTWOOP.\")\n",
    "        input(\"Press Enter to exit...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during startup: {e}\")\n",
    "    input(\"Press Enter to exit...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make startup script executable\n",
    "!chmod +x {repo_dir}/startup.py\n",
    "\n",
    "print(\"\\nSetup complete! You can now run VASTWOOP using either:\\n\")\n",
    "print(f\"1. Python: python {repo_dir}/startup.py\")\n",
    "print(f\"2. Directly: {repo_dir}/startup.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
